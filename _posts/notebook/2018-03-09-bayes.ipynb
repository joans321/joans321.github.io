{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title: python 贝叶斯算法之垃圾邮件分类\n",
    "category: math\n",
    "tags: [math, ai, python]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概念\n",
    "\n",
    "贝叶斯定理是概率论中的一个定理，其数学公式如下：\n",
    "\n",
    "$$\n",
    "P(Y|X) = \\frac{P(Y)P(X|Y)}{P(X)}\n",
    "$$\n",
    "\n",
    "其中 `P(Y|X)` 是在X发生的情况下，Y发生的概率，也叫条件概率。\n",
    "\n",
    "贝叶斯公式的推导过程如下：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\because & P(Y, X) & = P(Y)P(X|Y) \\\\\n",
    "\\because & P(X, Y) & = P(X)P(Y|X) \\\\\n",
    "\\because & P(Y, X) & = P(X, Y) \\\\\n",
    "\\therefore & P(Y)P(X|Y) & = P(X)P(Y|X)\\\\\n",
    "\\therefore & P(Y|X) & = \\frac{P(Y)P(X|Y)}{P(X)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中 P(Y, X) 和 P(X, Y) 都表示事件 X 和事件 Y 同时发生的概率（即事件 X,Y 的联合概率）。\n",
    "\n",
    "如果集合 Y 由 N 种元素构成（$Y_1$ 到 $Y_n$），且集合 X 由所有 Y 的逆映射构成则：\n",
    "\n",
    "$$\n",
    "P(X) = \\sum_{i=1}^n P(Y_i)P(X|Y_i)\n",
    "$$\n",
    "\n",
    "所以贝叶斯的另一种表达方式（即全概率公式）是：\n",
    "\n",
    "$$\n",
    "P(Y_j|X) = \\frac{P(Y_j) P(X|Y_j)}{\\sum_{i=1}^n P(Y_i)P(X|Y_i)}\n",
    "$$\n",
    "\n",
    "朴素贝叶斯是在贝叶斯定理基础之上，假设特征条件之间是相互独立的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 建模\n",
    "\n",
    "我们知道了贝叶斯公式，该如何利用贝叶斯来识别垃圾邮件呢？根据 [Paul Graham 的文章](http://www.paulgraham.com/spam.html) 我们知道利用邮件的单词作为集合 X 就可以利用贝叶斯公式推断出结果 Y 是否属于垃圾邮件。\n",
    "\n",
    "我们知道邮件分类的结果就两种:垃圾邮件(Spam)和正常邮件(Ham)。所以在开始之前先定义以下几个符号：\n",
    "\n",
    "* $P(Y=S)$ : 垃圾邮件的概率\n",
    "* $P(Y=H)$ : 正常邮件的概率\n",
    "* $P(W_i\\|Y=S)$ : 是垃圾邮件时，出现单词 $W_i$ 的概率\n",
    "* $P(W_i\\|Y=H)$ : 是正常邮件时，出现单词 $W_i$ 的概率\n",
    "* $P(Y=S \\| W_i)$ : 出现单词 $W_i$ 时，是垃圾邮件的概率\n",
    "\n",
    "我们收到一份邮件之后，可以识别出邮件中包含的所有单词，所以需要计算的是 `P(Y=S|W)` 的概率（即每个单词出现时是垃圾邮件的概率）。根据贝叶斯全概率公式可以得出：\n",
    "\n",
    "$$\n",
    "P(Y=S|W) = \\frac{P(Y=S)P(W|Y=S)}{P(Y=S)P(W|Y=S) + P(Y=H)P(W|Y=H)}\n",
    "$$\n",
    "\n",
    "上述公式中需要计算垃圾邮件、正常邮件的概率以及相应条件下出现各个单词的概率，这些都可以从训练数据中获取。\n",
    "\n",
    "比如收到的 2500 封邮件中有 779 封是垃圾邮件，则 `P(Y=S) = 0.3116`, `P(Y=H) = 0.6884`。 \n",
    "\n",
    "又比如，收到的正常邮件和垃圾邮件中分别包含 `sex` 的单词的概率分别为 `0.0005` 和 `0.05` 则对应可以求出:\n",
    "\n",
    "$$\n",
    "P(Y=S|W=sex) = \\frac{0.3116 * 0.05}{0.3116 * 0.05 + 0.6884 * 0.0005} = 0.9783\n",
    "$$\n",
    "\n",
    "现在我们可以使用以下步骤使用训练集的数据求出所有单词对应垃圾邮件的概率：\n",
    "1. 提供两组识别好的邮件数据（正常邮件和垃圾邮件）\n",
    "2. 计算每个单词在正常邮件和垃圾邮件中的出现频率\n",
    "3. 分别计算出垃圾邮件的封数以及单词总数、正常邮件的封数以及单词总数\n",
    "4. 根据上述公式算出所有 `P(Y=S|W)`\n",
    "\n",
    "训练和测试数据来自 [kaggle 垃圾邮件分类任务](https://www.kaggle.com/c/adcg-ss14-challenge-02-spam-mails-detection)，其中训练数据包含 1721 封正常邮件和 779 封垃圾邮件，测试数据同时包含 1827 封正常邮件和垃圾邮件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码是用来读取邮件文件数据并计数生成 pandas 的 DataFrame，训练集邮件点击[这里下载](https://github.com/joans321/joans321.github.io/tree/master/_posts/notebook/emails)，其中 TR 目录是训练邮件，TT 目录是测试邮件。主要实现功能如下：\n",
    "\n",
    "1. 遍历所有训练集的邮件\n",
    "2. 对邮件内容进行分词\n",
    "3. 根据邮件标签统计单词出现次数、总单词数以及邮件封数\n",
    "4. 把上述信息转换成数据帧和Hash表返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train email info :  {'ham_word_count': 448008, 'spam_word_count': 349625, 'file_count': 2500, 'ham_file_count': 1721, 'spam_file_count': 779}\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "import os, sys, stat\n",
    "import email\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_file(filename):\n",
    "    '''\n",
    "    读出邮件内容，需要使用 email 解析\n",
    "    :param filename: 邮件文件路径\n",
    "    :return: 返回邮件主题和邮件内容字符串, 可能带 html 格式\n",
    "    '''\n",
    "    with open(filename, encoding='latin-1') as fp:\n",
    "        msg = email.message_from_file(fp)\n",
    "        payload = msg.get_payload()\n",
    "        if type(payload) == type(list()):\n",
    "            payload = payload[0]\n",
    "        if type(payload) != type(''):\n",
    "            payload = str(payload)\n",
    "            \n",
    "        sub = msg.get('subject')\n",
    "        sub = str(sub)\n",
    "        return sub + payload\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    '''\n",
    "    清除邮件内容中的 html 标签\n",
    "    :param raw_html: 带 html 标签的文本内容\n",
    "    :return: 不带 html 标签的文本内容\n",
    "    '''\n",
    "    return raw_html # BeautifulSoup(raw_html, 'html.parser').text\n",
    "\n",
    "def label_from_file(filename):\n",
    "    '''\n",
    "    从文件名中读取需要，如 'TRAIN_1234.eml' 的文件序号为 1234\n",
    "    :param filename: 文件名\n",
    "    :return: 文件序号\n",
    "    '''\n",
    "    for s in re.findall(r'\\d+', filename):\n",
    "        return int(s)\n",
    "    raise ValueError('filename error : ' + filename)\n",
    "\n",
    "def calc_tf_idf(tf, idf, text, ignore=3):\n",
    "    '''\n",
    "    计算一份邮件内容的词频和逆文档频率（仅计数）\n",
    "    :param tf: 词频计数\n",
    "    :param idf: 逆文档频率计数\n",
    "    :return: 文档的单词数\n",
    "    '''\n",
    "    words = re.findall('\\w+', text)\n",
    "    count = 0\n",
    "    word_set = set()\n",
    "    for word in words:\n",
    "        # 过滤无效的单词\n",
    "        if len(word) < ignore or len(word) > 20:\n",
    "            continue\n",
    "        word = word.lower()\n",
    "        \n",
    "        # 统计逆文档频率, 一篇文章只加一次\n",
    "        if not (word in word_set):\n",
    "            idf[word] = idf.get(word, 0) + 1\n",
    "            word_set.add(word)\n",
    "            \n",
    "        # 统计词频\n",
    "        tf[word] = tf.get(word, 0) + 1\n",
    "        \n",
    "        # 计算一篇文档的单词总数\n",
    "        count = count + 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "def get_label(labels, index):\n",
    "    '''\n",
    "    获取邮件的标签\n",
    "    :param labels: 全部标签数据(Id 和 Prediction 两列)\n",
    "    :return: 1 表示正常邮件，0 表示垃圾邮件\n",
    "    '''\n",
    "    return labels.Prediction[labels.Id == index].iloc[0]\n",
    "\n",
    "def train_data():\n",
    "    '''\n",
    "    读取训练目录下到所有邮件和邮件分类标签\n",
    "    :return: 所有词频和逆文档频率和邮件数量信息\n",
    "    '''\n",
    "    pathname = 'emails/TR'\n",
    "    labels = pd.read_csv('emails/spam-mail.tr.label')\n",
    "    \n",
    "    ham_tf = dict()\n",
    "    spam_tf = dict()\n",
    "    word_idf = dict()\n",
    "    ham_word_count = 0\n",
    "    spam_word_count = 0\n",
    "    file_count = 0\n",
    "    spam_file_count = 0\n",
    "    ham_file_count = 0\n",
    "    \n",
    "    # 遍历所有邮件文件\n",
    "    for file in os.listdir(pathname):\n",
    "        fpath = os.path.join(pathname, file)\n",
    "        info = os.stat(fpath)\n",
    "        if stat.S_ISREG(info.st_mode) and file.endswith('.eml'):\n",
    "            '''\n",
    "            1. 从邮件文件出读出所有文本\n",
    "            2. 根据邮件标签，分别计算垃圾邮件的词频和逆文档频率\n",
    "            '''\n",
    "            text = clean_html(read_file(fpath))\n",
    "            index = label_from_file(file)\n",
    "            file_count = file_count + 1\n",
    "            if get_label(labels, index) == 1:\n",
    "                ham_file_count = ham_file_count + 1\n",
    "                ham_word_count = ham_word_count + calc_tf_idf(ham_tf, word_idf, text)\n",
    "            else:\n",
    "                spam_file_count = spam_file_count + 1\n",
    "                spam_word_count = spam_word_count + calc_tf_idf(spam_tf, word_idf, text)\n",
    "\n",
    "    info = {}\n",
    "    info['ham_word_count'] = ham_word_count\n",
    "    info['spam_word_count'] = spam_word_count\n",
    "    info['file_count'] = file_count\n",
    "    info['ham_file_count'] = ham_file_count\n",
    "    info['spam_file_count'] = spam_file_count\n",
    "    print('train email info : ', info)\n",
    "\n",
    "    word_df = pd.DataFrame([ham_tf, spam_tf, word_idf]).T\n",
    "    word_df.columns = ['ham_tf', 'spam_tf', 'word_idf']\n",
    "    return (word_df, info)\n",
    "\n",
    "'''\n",
    "读取所有训练集中的邮件，范围正常邮件和垃圾邮件对应每个词出现的次数以及训练集邮件的计数信息\n",
    "'''\n",
    "email_df, email_info = train_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码把训练集中的正常邮件和垃圾邮件的信息都统计出来了。其中 `email_df` 是一个二维数据，每行代表一个词，`spam_tf` 这列代表单词在垃圾邮件出现的次数，`ham_tf` 这列代表单词在正常邮件出现的次数。 下面要根据公式\n",
    "\n",
    "$$\n",
    "P(Y=S|W) = \\frac{P(Y=S)P(W|Y=S)}{P(Y=S)P(W|Y=S) + P(Y=H)P(W|Y=H)}\n",
    "$$\n",
    "\n",
    "计算出每个单词出现时是垃圾邮件的概率，并选择概率大于 90% 的词作为识别关键词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ham_tf   spam_tf  word_idf   spam_sp\n",
      "hibody         0.000002  0.001176     214.0  0.995823\n",
      "111n           0.000002  0.001090       8.0  0.995495\n",
      "0px            0.000004  0.001779     204.0  0.994487\n",
      "111r           0.000002  0.000887       7.0  0.994469\n",
      "11px           0.000002  0.000775     107.0  0.993678\n",
      "1px            0.000002  0.000664      62.0  0.992623\n",
      "111l           0.000002  0.000521       7.0  0.990616\n",
      "97n            0.000002  0.000443       7.0  0.988999\n",
      "xmlns          0.000002  0.000440     150.0  0.988929\n",
      "xhtml1         0.000004  0.000818     148.0  0.988087\n",
      "0in            0.000002  0.000398       5.0  0.987748\n",
      "ptsize         0.000002  0.000386      10.0  0.987390\n",
      "0pt            0.000002  0.000383      12.0  0.987297\n",
      "islands        0.000002  0.000366      17.0  0.986710\n",
      "115            0.000018  0.002883      26.0  0.986501\n",
      "97t            0.000002  0.000360       7.0  0.986501\n",
      "enenkio        0.000002  0.000355       1.0  0.986287\n",
      "20px           0.000002  0.000349      53.0  0.986065\n",
      "97nd           0.000002  0.000343       7.0  0.985836\n",
      "2px            0.000002  0.000340      38.0  0.985719\n",
      "padding        0.000018  0.002640     144.0  0.985277\n",
      "mso            0.000002  0.000326       6.0  0.985102\n",
      "111bject       0.000002  0.000320       7.0  0.984840\n",
      "normal__char   0.000002  0.000297       1.0  0.983693\n",
      "97l            0.000002  0.000295       7.0  0.983537\n",
      "15px           0.000002  0.000289      40.0  0.983216\n",
      "700            0.000002  0.000277      44.0  0.982536\n",
      "5px            0.000007  0.000767      93.0  0.981066\n",
      "doctype        0.000007  0.000732     259.0  0.980196\n",
      "13px           0.000002  0.000243      43.0  0.980120\n",
      "...                 ...       ...       ...       ...\n",
      "3b6797         0.000002  0.000046       1.0  0.902726\n",
      "medievalmedia  0.000002  0.000046       2.0  0.902726\n",
      "skonibut       0.000002  0.000046       1.0  0.902726\n",
      "mortgages101   0.000002  0.000046       2.0  0.902726\n",
      "23ffffff       0.000002  0.000046       3.0  0.902726\n",
      "225            0.000002  0.000046       7.0  0.902726\n",
      "sorowxf        0.000002  0.000046       1.0  0.902726\n",
      "equivalence    0.000002  0.000046       2.0  0.902726\n",
      "97cteri        0.000002  0.000046       7.0  0.902726\n",
      "medicine       0.000002  0.000046      15.0  0.902726\n",
      "eid            0.000002  0.000046       5.0  0.902726\n",
      "97lled         0.000002  0.000046       7.0  0.902726\n",
      "187            0.000002  0.000046       5.0  0.902726\n",
      "tabdesmond61a  0.000002  0.000046       2.0  0.902726\n",
      "97me           0.000002  0.000046       7.0  0.902726\n",
      "dd3960         0.000002  0.000046       2.0  0.902726\n",
      "abolished      0.000002  0.000046      10.0  0.902726\n",
      "ght            0.000002  0.000046      10.0  0.902726\n",
      "97u            0.000002  0.000046       7.0  0.902726\n",
      "taongi         0.000002  0.000046       1.0  0.902726\n",
      "97ttered       0.000002  0.000046       7.0  0.902726\n",
      "000066         0.000002  0.000046       5.0  0.902726\n",
      "118ed          0.000002  0.000046       7.0  0.902726\n",
      "118ely         0.000002  0.000046       7.0  0.902726\n",
      "118i           0.000002  0.000046       7.0  0.902726\n",
      "97ngle         0.000002  0.000046       7.0  0.902726\n",
      "118iewing      0.000002  0.000046       7.0  0.902726\n",
      "97ncy          0.000002  0.000046       7.0  0.902726\n",
      "heat           0.000013  0.000272      23.0  0.901803\n",
      "farmer         0.000007  0.000134      13.0  0.900862\n",
      "\n",
      "[530 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 拷贝数据，可重复运行这段代码\n",
    "word_df = email_df.copy()\n",
    "word_df.fillna(1, inplace=True)    \n",
    "\n",
    "# P(Y=S) : 垃圾邮件的概率\n",
    "p_y_s = email_info['spam_file_count'] /  email_info['file_count']\n",
    "\n",
    "# P(Y=H) : 正常邮件的概率\n",
    "p_y_h = 1 - p_y_s\n",
    "\n",
    "# P(W|Y=H) : 正常邮件时，出现单词 W 的概率\n",
    "word_df['ham_tf'] = word_df['ham_tf'] / email_info['ham_word_count']\n",
    "\n",
    "# P(W|Y=S) : 垃圾邮件时，出现单词 W 的概率\n",
    "word_df['spam_tf'] = word_df['spam_tf'] / email_info['spam_word_count']\n",
    "\n",
    "# 根据公式计算 P(Y=S|W)\n",
    "word_df['spam_sp'] = (word_df['spam_tf'] * p_y_s) / (word_df['ham_tf'] * p_y_h + word_df['spam_tf'] * p_y_s)\n",
    "\n",
    "# 根据公式计算 P(Y=H|W)\n",
    "# word_df['spam_hp'] = (word_df['ham_tf'] * p_y_h) / (word_df['ham_tf'] * p_y_h + word_df['spam_tf'] * p_y_s)\n",
    "\n",
    "# 选择 P(Y=S|W) >= 0.9 的单词作为识别关键词，节省计算\n",
    "word_df = word_df.loc[(word_df['spam_sp'] >= 0.9)]\n",
    "\n",
    "# 从大到小排序\n",
    "word_df = word_df.sort_values(by=['spam_sp'], ascending=[False])\n",
    "\n",
    "print(word_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里为止，我们已经利用贝叶斯算出了每个单词对应垃圾邮件的概率。我们知道，邮件是由很多个词构成的，一个单词还不足以判断是否为垃圾邮件。所以下面还需要用到朴素贝叶斯方法（即假设每个词是相互独立的）。假设一封邮件有 n 个单词，则这封邮件是垃圾邮件的概率为：\n",
    "\n",
    "$$\n",
    "P(Y=S|W_1, ... , W_n) = \\frac{P(W_i, ... , W_n|Y=S)P(Y=S)}{P(W_i, ... , W_n|Y=S)P(Y=S) + P(W_i, ... , W_n|Y=H)P(Y=H)}\n",
    "$$\n",
    "\n",
    "假设每个词是独立概率分布，则：\n",
    "\n",
    "$$\n",
    "P(W_i, ... , W_n|Y=S) = P(W_1|Y=S) P(W_2|Y=S) ... P(W_n|Y=S)\n",
    "$$\n",
    "\n",
    "同理可得：\n",
    "\n",
    "$$\n",
    "P(Y=S|W_1, ... , W_n) = \\frac{P(W_1|Y=S)...P(W_n|Y=S)P(Y=S)}{P(W_1|Y=S)...P(W_n|Y=S)P(Y=S) + P(W_1|Y=H)...P(W_n|Y=H)P(Y=H)}\n",
    "$$\n",
    "\n",
    "到这里，我们已经把整个邮件所有单词对应垃圾邮件的联合概率转换成单词的条件概率乘积了。由于邮件中的单词数比较多，我们只挑选比较可能是垃圾邮件的词来计算，以提高运算效率。下面我们用代码读出测试邮件并计算每封邮件是垃圾邮件的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emal count ham 1178 spam 649\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def is_spam_email(filename, word_df, info, ignore=3):\n",
    "    text = clean_html(read_file(filename))\n",
    "    words = re.findall('[A-Za-z]+', text)\n",
    "    word_set = set()\n",
    "    p_s_w = info['spam_file_count'] /  info['file_count']\n",
    "    p_h_w = 1 - p_s_w\n",
    "    \n",
    "    for word in words:\n",
    "        # 过滤无效的单词\n",
    "        if len(word) < ignore or len(word) > 20:\n",
    "            continue\n",
    "            \n",
    "        word = word.lower()\n",
    "\n",
    "        # 属于垃圾邮件关键词 且 未参与计算过, 分子分母都乘以1000，防止小数点过小导致计算结果为0\n",
    "        if (word in word_df.index) and not (word in word_set):\n",
    "            word_set.add(word)\n",
    "            p_s_w = 1000 * p_s_w * (word_df.loc[word].spam_tf)\n",
    "            p_h_w = 1000 * p_h_w * (word_df.loc[word].ham_tf)\n",
    "            \n",
    "\n",
    "    # 没有垃圾邮件关键词则认为是正常邮件\n",
    "    if len(word_set) == 0:\n",
    "        return (False, 0)\n",
    "    \n",
    "    # print('file %s p_s_w : %f, p_h_w %f, word count %d' % (filename, p_s_w, p_h_w, len(word_set)))\n",
    "    result = p_s_w / (p_s_w + p_h_w)\n",
    "    if result > 0.9:\n",
    "        return (True, result)\n",
    "    return (False, result)\n",
    "\n",
    "def test_data():\n",
    "    pathname = 'emails/TT'\n",
    "    spam_count = 0\n",
    "    ham_count = 0\n",
    "    Id = []\n",
    "    Prediction = []\n",
    "    \n",
    "    # 遍历所有邮件文件\n",
    "    for file in os.listdir(pathname):\n",
    "        fpath = os.path.join(pathname, file)\n",
    "        info = os.stat(fpath)\n",
    "        if stat.S_ISREG(info.st_mode) and file.endswith('.eml'):\n",
    "            spam, p = is_spam_email(fpath, word_df, email_info)\n",
    "            value = 0 if spam else 1\n",
    "            index = label_from_file(fpath)\n",
    "            Id.append(index)\n",
    "            Prediction.append(value)\n",
    "            if spam:\n",
    "                spam_count = spam_count + 1\n",
    "                # print('email %s is %s and p %f' % (fpath, value, p))\n",
    "            else:\n",
    "                ham_count = ham_count + 1\n",
    "    \n",
    "    print('emal count ham %d spam %d' % (ham_count, spam_count))\n",
    "    return (Id, Prediction)\n",
    "\n",
    "# 执行测试\n",
    "# is_spam_email('emails/TT/TEST_117.eml', word_df, email_info)\n",
    "# is_spam_email('emails/TT/TEST_998.eml', word_df, email_info)\n",
    "# is_spam_email('emails/TT/TEST_1302.eml', word_df, email_info)\n",
    "Id, Prediction = test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "根据返回的 Id 和 预测结果，生成 DataFrame 并排序后写到 csv 文件中\n",
    "生的 csv 文件结果可以直接提交到 kaggle\n",
    "'''\n",
    "test_df = pd.DataFrame(data=np.array([Id, Prediction])).T\n",
    "test_df.columns = ['Id', 'Prediction']\n",
    "test_df = test_df.sort_values(by=['Id'], ascending=[True])\n",
    "test_df.to_csv('emails/submission.csv', sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码根据朴素贝叶斯算法测试出 `1827` 封邮件中有 649 封是垃圾邮件并且把测试结果写到 `submission.csv` 中。这里我们已经完成了贝叶斯算法识别垃圾邮件的 Python 代码编码，上面的代码可以直接在 jupyter notebook 中打开运行，方便大家参考。\n",
    "\n",
    "`后记：上述基础的朴素贝叶斯算法生成的测试结果在 kaggle 得分是 0.83470`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "\n",
    "1. [贝叶斯推断及其互联网应用（二）：过滤垃圾邮件](http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_two.html)\n",
    "2. [机器学习算法系列（10）：朴素贝叶斯](https://plushunter.github.io/2017/02/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/)\n",
    "3. [TF-IDF与余弦相似性的应用（一）：自动提取关键词](http://www.ruanyifeng.com/blog/2013/03/tf-idf.html)\n",
    "4. [数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
